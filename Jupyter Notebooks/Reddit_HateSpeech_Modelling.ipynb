{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/radonys/Reddit-HateSpeech-Application/blob/master/Reddit_HateSpeech_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtwF1vxIbRkf"
   },
   "source": [
    "# Reddit Hate-Speech Modelling\n",
    "\n",
    "Reddit Hate-Speech Modelling using data from [A Benchmark Dataset for Learning to Intervene in Online Hate Speech](https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0VbgaQdb_p8"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKSsiOeqcDJ9"
   },
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "uRERImIkcGWE",
    "outputId": "95d79eb4-d97d-4d4e-cc68-8c99def1f038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/39/17251486951815d4514e4a3f179d4f3e7af5f7b1ce8eaba5a3ea61bc91f2/praw-7.0.0-py3-none-any.whl (143kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 860kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting websocket-client>=0.54.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 815kB/s eta 0:00:01     |████████████████▎               | 102kB 815kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting update-checker>=0.16\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/c3/aaf8a162df8e8f9d321237c7c0e63aff95b42d19f1758f96606e3cabb245/update_checker-0.17-py2.py3-none-any.whl\n",
      "Collecting prawcore<2.0,>=1.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/8e/d076cb8f26523f91eef3e75d6cf9143b2f16d67ce7d681a61d0bbc783f49/prawcore-1.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: six in /Users/yashsrivastava/env3/lib/python3.7/site-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /Users/yashsrivastava/env3/lib/python3.7/site-packages (from update-checker>=0.16->praw) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/yashsrivastava/env3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yashsrivastava/env3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/yashsrivastava/env3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/yashsrivastava/env3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.24.3)\n",
      "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
      "Successfully installed praw-7.0.0 prawcore-1.3.0 update-checker-0.17 websocket-client-0.57.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWxZ-KZfbZem"
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jFHE0ZFtXzvf",
    "outputId": "d47accad-ce59-4867-e2ec-8f5d1e33bddc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yashsrivastava/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import logging\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVzfoxrHcZgA"
   },
   "source": [
    "## Variable Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLJX-hKPcbzN"
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='#', client_secret='#', user_agent='#', username='#', password='#')\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfpFYM6fceg1"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrUiJcy4ck_J"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "   \n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = text.lower()\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "\n",
    "    return text\n",
    "\n",
    "def process_info(row):\n",
    "\n",
    "  info = defaultdict(list)\n",
    "\n",
    "  texts = row['text'].split(\"\\n\")[:-1]\n",
    "  \n",
    "  hate_idx = row['hate_speech_idx'][1:-1].split(',')\n",
    "  hate_idx = [int(i) - 1 for i in hate_idx]\n",
    "\n",
    "  for txt in texts:\n",
    "    info['text'].append(clean_text(txt))\n",
    "\n",
    "  hate = np.zeros(len(texts))\n",
    "\n",
    "  try:\n",
    "    \n",
    "    for idx in hate_idx:\n",
    "      hate[idx] = 1\n",
    "\n",
    "  except Exception as error:\n",
    "    \n",
    "    print(error)\n",
    "    return {}\n",
    "\n",
    "  info['hate'] = hate\n",
    "\n",
    "  return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gYAf8cCcrEf"
   },
   "source": [
    "## Download Annotated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYvpgHTfd2-d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-17 01:59:02--  https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech/raw/master/data/reddit.csv\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech/master/data/reddit.csv [following]\n",
      "--2020-05-17 01:59:04--  https://raw.githubusercontent.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech/master/data/reddit.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.248.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.248.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7384220 (7.0M) [text/plain]\n",
      "Saving to: ‘reddit.csv’\n",
      "\n",
      "reddit.csv          100%[===================>]   7.04M   684KB/s    in 7.9s    \n",
      "\n",
      "2020-05-17 01:59:12 (911 KB/s) - ‘reddit.csv’ saved [7384220/7384220]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hate_speech_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. A subsection of retarded Hungarians? Ohh bo...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. &gt; \"y'all hear sumn?\"  by all means I live i...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1. Because the Japanese aren't retarded and kn...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1. That might be true if we didn't have an exa...</td>\n",
       "      <td>[2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. Why, what is the point of making all of tha...</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text hate_speech_idx\n",
       "0  1. A subsection of retarded Hungarians? Ohh bo...             [1]\n",
       "1  1. > \"y'all hear sumn?\"  by all means I live i...             [3]\n",
       "2  1. Because the Japanese aren't retarded and kn...             [1]\n",
       "3  1. That might be true if we didn't have an exa...          [2, 3]\n",
       "4  1. Why, what is the point of making all of tha...             [8]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech/raw/master/data/reddit.csv\n",
    "data = pd.read_csv(\"reddit.csv\")\n",
    "data.dropna(subset = ['hate_speech_idx'], inplace = True)\n",
    "data.drop(['response', 'id'], inplace = True, axis = 1)\n",
    "data.reset_index(inplace = True, drop = True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDe8PJylokXw"
   },
   "source": [
    "## Clean Downloaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "s87mffXronC3",
    "outputId": "54cd2daf-0ac8-4d07-fe56-330bbf6dc5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 2 is out of bounds for axis 0 with size 2\n",
      "index 19 is out of bounds for axis 0 with size 14\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = pd.DataFrame()\n",
    "\n",
    "for row in data.iterrows():\n",
    "  cleaned_data = cleaned_data.append(pd.DataFrame(process_info(row[1])), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJzyEGUI07_C"
   },
   "source": [
    "## Hate/Non-Hate Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g4UuOXtX1KB9"
   },
   "source": [
    "### ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CvkDA-Ki1OUo"
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjhsMiRY0oCH"
   },
   "outputs": [],
   "source": [
    "def logisticreg(X_train, X_test, y_train, y_test):\n",
    "\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "  logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                 ])\n",
    "  logreg.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = logreg.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSFZXfVt1Xks"
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmwHSm8M1WtQ"
   },
   "outputs": [],
   "source": [
    "def randomforest(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  \n",
    "  ranfor = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', RandomForestClassifier(n_estimators = 1000, random_state = 42)),\n",
    "                 ])\n",
    "  ranfor.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = ranfor.predict(X_test)\n",
    "\n",
    "  filename = 'finalized_model.sav'\n",
    "  pickle.dump(ranfor, open(filename, 'wb'))\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ob2uCTp1eWf"
   },
   "source": [
    "#### Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAb_F4cQ1iBU"
   },
   "outputs": [],
   "source": [
    "def linear_svm(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "  from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "  sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "                 ])\n",
    "  sgd.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = sgd.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YNW4harx1vO8"
   },
   "source": [
    "### Train Test Varied Data ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akxWWIYd1zQZ"
   },
   "outputs": [],
   "source": [
    "def train_test(X,y):\n",
    " \n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "  print(\"Results of Linear Support Vector Machine\")\n",
    "  linear_svm(X_train, X_test, y_train, y_test)\n",
    "  print(\"Results of Logistic Regression\")\n",
    "  logisticreg(X_train, X_test, y_train, y_test)\n",
    "  print(\"Results of Random Forest\")\n",
    "  randomforest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "0yWiof-K19jY",
    "outputId": "54e04b70-3e23-47e0-8ef5-84f733bb6ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Linear Support Vector Machine\n",
      "accuracy 0.8224666142969363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.99      0.89      3542\n",
      "         1.0       0.96      0.43      0.60      1550\n",
      "\n",
      "    accuracy                           0.82      5092\n",
      "   macro avg       0.88      0.71      0.74      5092\n",
      "weighted avg       0.85      0.82      0.80      5092\n",
      "\n",
      "Results of Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashsrivastava/env3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7360565593087196\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81      3542\n",
      "         1.0       0.56      0.62      0.59      1550\n",
      "\n",
      "    accuracy                           0.74      5092\n",
      "   macro avg       0.69      0.70      0.70      5092\n",
      "weighted avg       0.74      0.74      0.74      5092\n",
      "\n",
      "Results of Random Forest\n",
      "accuracy 0.9183032207384132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.97      0.94      3542\n",
      "         1.0       0.93      0.79      0.85      1550\n",
      "\n",
      "    accuracy                           0.92      5092\n",
      "   macro avg       0.92      0.88      0.90      5092\n",
      "weighted avg       0.92      0.92      0.92      5092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test(cleaned_data.text, cleaned_data.hate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEQ6wB4v64wV"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shGjLmfu68XX"
   },
   "source": [
    "1) https://arxiv.org/abs/1909.04251\n",
    "\n",
    "2) https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech\n",
    "\n",
    "3) http://www.storybench.org/how-to-scrape-reddit-with-python/\n",
    "\n",
    "4) https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOBoyVukntjPfFLQD4JzRvS",
   "collapsed_sections": [
    "R0VbgaQdb_p8",
    "vKSsiOeqcDJ9",
    "CWxZ-KZfbZem",
    "eVzfoxrHcZgA",
    "LfpFYM6fceg1",
    "9gYAf8cCcrEf",
    "WDe8PJylokXw",
    "CvkDA-Ki1OUo",
    "6Ob2uCTp1eWf",
    "YNW4harx1vO8"
   ],
   "include_colab_link": true,
   "name": "Reddit-HateSpeech-Modelling.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
